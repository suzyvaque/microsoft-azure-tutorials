{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e15e24d8",
      "metadata": {},
      "source": [
        "# üçè Observability & Tracing Demo with `azure-ai-projects` and `azure-ai-inference` üçé\n",
        "\n",
        "Welcome to this **Health & Fitness**-themed notebook, where we'll explore:\n",
        "\n",
        "1. **Getting Model Info** with an `AIProjectClient`\n",
        "2. **Listing Connections** to show how we can manage and check all our resources\n",
        "3. **Observability** and tracing examples, showing how to set up:\n",
        "   - Console tracing (OpenTelemetry logs printed to stdout)\n",
        "   - Azure Monitor tracing (sending your logs to an Application Insights resource)\n",
        "   - Viewing your traces in **Azure AI Foundry** üéâ\n",
        "\n",
        "> **Disclaimer**: This is a fun demonstration of AI and observability! Any references to workouts, diets, or health routines in the code or prompts are purely for **educational** purposes. Always consult a professional for health advice. üôå\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8bb63be",
      "metadata": {},
      "source": [
        "\n",
        "<img src=\"./seq-diagrams/1-observability.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65d3f516",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports üõ†Ô∏è\n",
        "In this step, we'll load environment variables (like `PROJECT_CONNECTION_STRING`), then initialize the **`AIProjectClient`**. We'll confirm we can retrieve **model info**. The sample environment variables are typically stored in an `.env` file or in your shell environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c385cb21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import UserMessage\n",
        "from pathlib import Path  # For cross-platform path handling\n",
        "\n",
        "# Get the path to the .env file which is in the parent directory\n",
        "notebook_path = Path().absolute()  # Get absolute path of current notebook\n",
        "parent_dir = notebook_path.parent  # Get parent directory\n",
        "load_dotenv(parent_dir / '.env')  # Load environment variables from .env file\n",
        "\n",
        "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "if not connection_string:\n",
        "    raise ValueError(\"üö® PROJECT_CONNECTION_STRING not found in environment. Please set it in your .env.\")\n",
        "\n",
        "try:\n",
        "    # Create the AIProjectClient\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=connection_string\n",
        "    )\n",
        "    print(\"‚úÖ Successfully created AIProjectClient\")\n",
        "    \n",
        "    # Get chat completions client and make request\n",
        "    with project_client.inference.get_chat_completions_client() as inference_client:\n",
        "        response = inference_client.complete(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),  # Get model name from env or use default\n",
        "            messages=[UserMessage(content=\"How many feet are in a mile?\")]\n",
        "        )\n",
        "        print(\"üí° Response:\")\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Failed to initialize client or get response:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03514f9d",
      "metadata": {},
      "source": [
        "## 2. List & Inspect Connections üîå\n",
        "We'll now demonstrate how to **list connections** in your AI Foundry project. This can help you see all the resources connected, or just a subset (like `AZURE_OPEN_AI` connections).\n",
        "\n",
        "*Note*: We'll just print them out so you can see the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b39a5984",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import ConnectionType\n",
        "\n",
        "with project_client:\n",
        "    # List all connections\n",
        "    all_conns = project_client.connections.list()\n",
        "    print(f\"üîé Found {len(all_conns)} total connections.\")\n",
        "    for idx, c in enumerate(all_conns):\n",
        "        print(f\"{idx+1}) Name: {c.name}, Type: {c.type}, IsDefault: {c.is_default}\")\n",
        "\n",
        "    # Filter for Azure OpenAI type, as an example\n",
        "    aoai_conns = project_client.connections.list(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
        "    print(f\"\\nüåÄ Found {len(aoai_conns)} Azure OpenAI connections:\")\n",
        "    for c in aoai_conns:\n",
        "        print(f\"   -> {c.name}\")\n",
        "\n",
        "    # Get the default Azure AI Services connection\n",
        "    default_conn = project_client.connections.get_default(\n",
        "        connection_type=ConnectionType.AZURE_AI_SERVICES,\n",
        "        include_credentials=False\n",
        "    )\n",
        "    if default_conn:\n",
        "        print(\"\\n‚≠ê Default Azure AI Services connection:\")\n",
        "        print(default_conn)\n",
        "    else:\n",
        "        print(\"No default connection found for Azure AI Services.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90145015",
      "metadata": {},
      "source": [
        "## 3. Observability & Tracing üåê\n",
        "\n",
        "### 3.1 Console Tracing Example\n",
        "We'll set up **console** tracing with the `opentelemetry` library so that logs are printed to `sys.stdout`. This is helpful for local debugging or minimal setups. We'll do a small chat completion example for fun (like asking a health question)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c389dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from azure.ai.inference.models import UserMessage\n",
        "from opentelemetry import trace\n",
        "\n",
        "# We'll enable local console tracing so we can see the telemetries in our terminal\n",
        "project_client.telemetry.enable(destination=sys.stdout)\n",
        "\n",
        "# We'll do a small LLM call example:\n",
        "try:\n",
        "    with project_client.inference.get_chat_completions_client() as client:\n",
        "        prompt_msg = \"I'd like to start a simple home workout routine. Any tips?\"\n",
        "        response = client.complete(\n",
        "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"some-deployment-name\"),\n",
        "            messages=[UserMessage(content=prompt_msg)]\n",
        "        )\n",
        "        print(\"\\nü§ñ Response:\", response.choices[0].message.content)\n",
        "except Exception as exc:\n",
        "    print(f\"‚ùå Chat Completions example failed: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e07fa7",
      "metadata": {},
      "source": [
        "### 3.2 Azure Monitor Tracing Example\n",
        "Now, instead of just console logs, we can push these logs to **Application Insights** (Azure Monitor) for deeper **APM** (application performance monitoring) and persistent logs.\n",
        "\n",
        "In order to do this, ensure you have an Application Insights **Connection String** associated with your AI Foundry project. Then configure your local environment to pull that connection string and set up `opentelemetry` for remote ingestion.\n",
        "\n",
        "We'll do a quick demonstration of how to do that (similar to the official sample)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b7d563",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.monitor.opentelemetry import configure_azure_monitor\n",
        "from azure.ai.inference.models import UserMessage\n",
        "\n",
        "# Enable Azure Monitor tracing if available\n",
        "connection_str = project_client.telemetry.get_connection_string()\n",
        "if connection_str:\n",
        "    print(\"üîß Found App Insights connection string. Configuring...\")\n",
        "    configure_azure_monitor(connection_string=connection_str)\n",
        "    project_client.telemetry.enable()  # add optional additional instrumentations\n",
        "    \n",
        "    # We'll do a test chat call again, which should get logged to Azure Monitor\n",
        "    try:\n",
        "        with project_client.inference.get_chat_completions_client() as client:\n",
        "            prompt_msg = \"Any low-impact exercises recommended for knee issues?\"\n",
        "            response = client.complete(\n",
        "                model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"some-deployment-name\"),\n",
        "                messages=[UserMessage(content=prompt_msg)]\n",
        "            )\n",
        "            print(\"\\nü§ñ Response (logged to App Insights):\", response.choices[0].message.content)\n",
        "    except Exception as exc:\n",
        "        print(f\"‚ùå Chat Completions with Azure Monitor example failed: {exc}\")\n",
        "else:\n",
        "    print(\"No Application Insights connection string is configured in this project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b572ab",
      "metadata": {},
      "source": [
        "## 4. Wrap-Up & Next Steps üéâ\n",
        "\n",
        "Congrats on exploring:\n",
        "1. Basic usage of **AIProjectClient** (model info, listing connections)\n",
        "2. **Observability** with console tracing\n",
        "3. **Application Insights**-based tracing for deeper logs & APM\n",
        "\n",
        "**Where to go next?**\n",
        "- **AI Foundry Portal**: Under the **Tracing** tab, you can see your traces in an easy UI.\n",
        "- **Azure Monitor**: Head into the Application Insights resource for advanced metrics, logs, and dashboards.\n",
        "- **azure-ai-evaluation**: Evaluate the quality of your LLM outputs, get scoring metrics, or embed it in your CI/CD pipeline.\n",
        "\n",
        "> üçÄ **Health Reminder**: All suggestions from the LLM are for demonstration only. Always consult professionals for health and fitness guidance.\n",
        "\n",
        "Enjoy building robust, observable GenAI apps! üèãÔ∏è‚Äç‚ôÇÔ∏è"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "name": "Observability_and_Tracing_Demo"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
