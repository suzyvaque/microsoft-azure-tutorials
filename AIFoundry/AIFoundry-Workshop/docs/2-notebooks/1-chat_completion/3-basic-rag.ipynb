{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cf62fbc",
      "metadata": {},
      "source": [
        "# 🍏 Basic Retrieval-Augmented Generation (RAG) with AIProjectClient 🍎\n",
        "\n",
        "In this notebook, we'll demonstrate a **basic RAG** flow using:\n",
        "- **`azure-ai-projects`** (AIProjectClient)\n",
        "- **`azure-ai-inference`** (Embeddings, ChatCompletions)\n",
        "- **`azure-ai-search`** (for vector or hybrid search)\n",
        "\n",
        "Our theme is **Health & Fitness** 🍏 so we’ll create a simple set of health tips, embed them, store them in a search index, then do a query that retrieves relevant tips, and pass them to an LLM to produce a final answer.\n",
        "\n",
        "> **Disclaimer**: This is not medical advice. For real health questions, consult a professional.\n",
        "\n",
        "## What is RAG?\n",
        "Retrieval-Augmented Generation (RAG) is a technique where the LLM (Large Language Model) uses relevant retrieved text chunks from your data to craft a final answer. This helps ground the model's response in real data, reducing hallucinations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26cbaa3",
      "metadata": {},
      "source": [
        "<img src=\"./seq-diagrams/3-basic-rag.png\" width=\"30%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99dfbd81",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n",
        "\n",
        "### Prerequisites\n",
        "1. Python 3.8+\n",
        "2. `pip install azure-ai-projects azure-ai-inference azure-search-documents azure-identity`\n",
        "3. `.env` with:\n",
        "   ```bash\n",
        "   PROJECT_CONNECTION_STRING=<your-conn-string>\n",
        "   MODEL_DEPLOYMENT_NAME=<some-chat-model>\n",
        "   SEARCH_INDEX_NAME=<your-search-index>\n",
        "   ```\n",
        "4. An **Azure AI Search** connection in your project, or any index ready to store embeddings.\n",
        "5. You also have a deployed LLM for chat + an embeddings model deployment (like `text-embedding-ada-002` or any other embedding model).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7395b2e",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# azure-ai-projects\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# We'll embed with azure-ai-inference\n",
        "from azure.ai.inference import EmbeddingsClient, ChatCompletionsClient\n",
        "from azure.ai.inference.models import UserMessage, SystemMessage\n",
        "\n",
        "# For vector search or hybrid search\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "conn_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "chat_model = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
        "search_index_name = os.environ.get(\"SEARCH_INDEX_NAME\", \"healthtips-index\")\n",
        "\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=conn_string,\n",
        "    )\n",
        "    print(\"✅ AIProjectClient created successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Error creating AIProjectClient:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b84bb8",
      "metadata": {},
      "source": [
        "## 2. Create Sample Health Data\n",
        "We'll create a few short doc chunks. In a real scenario, you might read from CSV or PDFs, chunk them up, embed them, and store them in your search index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eab53d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "health_tips = [\n",
        "    {\n",
        "        \"id\": \"doc1\",\n",
        "        \"content\": \"Daily 30-minute walks help maintain a healthy weight and reduce stress.\",\n",
        "        \"source\": \"General Fitness\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc2\",\n",
        "        \"content\": \"Stay hydrated by drinking 8-10 cups of water per day.\",\n",
        "        \"source\": \"General Fitness\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc3\",\n",
        "        \"content\": \"Consistent sleep patterns (7-9 hours) improve muscle recovery.\",\n",
        "        \"source\": \"General Fitness\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc4\",\n",
        "        \"content\": \"For cardio endurance, try interval training like HIIT.\",\n",
        "        \"source\": \"Workout Advice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc5\",\n",
        "        \"content\": \"Warm up with dynamic stretches before running to reduce injury risk.\",\n",
        "        \"source\": \"Workout Advice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc6\",\n",
        "        \"content\": \"Balanced diets typically include protein, whole grains, fruits, vegetables, and healthy fats.\",\n",
        "        \"source\": \"Nutrition\"\n",
        "    },\n",
        "]\n",
        "print(\"Created a small list of health tips.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29136555",
      "metadata": {},
      "source": [
        "## 3. Generate Embeddings + Store in Azure Search\n",
        "We'll show a minimal approach:\n",
        "1. Get embeddings client.\n",
        "2. Embed each doc.\n",
        "3. Upsert docs into Azure AI Search index with `embedding` field.\n",
        "\n",
        "### 3.1. Connect to Azure Search\n",
        "We'll do so by retrieving the default search connection from the project, then building a `SearchClient` from the `azure.search.documents` library.\n",
        "\n",
        "After that, we embed each doc, then upsert into your index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d0ef9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.projects.models import ConnectionType\n",
        "\n",
        "# Get the default search connection, with credentials\n",
        "search_conn = project_client.connections.get_default(\n",
        "    connection_type=ConnectionType.AZURE_AI_SEARCH, include_credentials=True\n",
        ")\n",
        "if not search_conn:\n",
        "    raise RuntimeError(\"❌ No default Azure AI Search connection found!\")\n",
        "\n",
        "search_client = SearchClient(\n",
        "    endpoint=search_conn.endpoint_url,\n",
        "    index_name=search_index_name,\n",
        "    credential=AzureKeyCredential(search_conn.key)\n",
        ")\n",
        "print(\"✅ Created Azure SearchClient.\")\n",
        "\n",
        "# Now create embeddings client\n",
        "embeddings_client = project_client.inference.get_embeddings_client()\n",
        "print(\"✅ Created embeddings client.\")\n",
        "\n",
        "search_docs = []\n",
        "for doc in health_tips:\n",
        "    # embed doc content\n",
        "    emb_response = embeddings_client.embed(\n",
        "        input=[doc[\"content\"]]\n",
        "    )\n",
        "    emb_vec = emb_response.data[0].embedding\n",
        "\n",
        "    # We'll build a doc with 'id', 'content', 'source', 'embedding'\n",
        "    search_docs.append(\n",
        "        {\n",
        "            \"id\": doc[\"id\"],\n",
        "            \"content\": doc[\"content\"],\n",
        "            \"source\": doc[\"source\"],\n",
        "            \"embedding\": emb_vec,\n",
        "        }\n",
        "    )\n",
        "\n",
        "result = search_client.upload_documents(documents=search_docs)\n",
        "print(f\"Uploaded {len(search_docs)} docs to Search index '{search_index_name}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05e5468",
      "metadata": {},
      "source": [
        "## 4. Basic RAG Flow\n",
        "### 4.1. Retrieve\n",
        "When a user queries, we:\n",
        "1. Embed user question.\n",
        "2. Search vector index with that embedding to get top docs.\n",
        "\n",
        "### 4.2. Generate answer\n",
        "We then pass the retrieved docs to the chat model.\n",
        "\n",
        "> In a real scenario, you'd have a more advanced approach to chunking & summarizing. We'll keep it simple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15c3aab",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_chat(query: str, top_k: int = 3) -> str:\n",
        "    # 1) Embed user query\n",
        "    user_vec = embeddings_client.embed(input=[query]).data[0].embedding\n",
        "\n",
        "    # 2) Vector search\n",
        "    # We'll assume your index has the vector field named 'embedding',\n",
        "    # using vector search or hybrid. We'll do a minimal example.\n",
        "\n",
        "    results = search_client.search(\n",
        "        search_text=\"\",  # for vector search we don't rely on textual search\n",
        "        vector=user_vec,\n",
        "        vector_fields=\"embedding\",\n",
        "        top=top_k\n",
        "    )\n",
        "\n",
        "    # gather the top docs\n",
        "    top_docs_content = []\n",
        "    for r in results:\n",
        "        # Each doc is a SearchResult object with doc: dict.\n",
        "        c = r[\"content\"]\n",
        "        s = r[\"source\"]\n",
        "        top_docs_content.append(f\"Source: {s} => {c}\")\n",
        "\n",
        "    # 3) Chat with retrieved docs.\n",
        "    # We'll pass a system message instructing the model to use them.\n",
        "    # We'll just do a minimal approach.\n",
        "\n",
        "    system_text = (\n",
        "        \"You are a health & fitness assistant.\\n\"\n",
        "        \"Answer user questions using ONLY the text from these docs.\\n\"\n",
        "        \"Docs:\\n\"\n",
        "        + \"\\n\".join(top_docs_content)\n",
        "        + \"\\nIf unsure, say 'I'm not sure'.\\n\"\n",
        "    )\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            model=chat_model,\n",
        "            messages=[\n",
        "                SystemMessage(content=system_text),\n",
        "                UserMessage(content=query)\n",
        "            ]\n",
        "        )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfecfb3c",
      "metadata": {},
      "source": [
        "## 5. Try a Query 🎉\n",
        "Let's do a question about cardio for busy people.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3937fdfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = \"What's a good short cardio routine for me if I'm busy?\"\n",
        "answer = rag_chat(user_query)\n",
        "print(\"🗣️ User Query:\", user_query)\n",
        "print(\"🤖 RAG Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e562e6",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "We've demonstrated a **basic RAG** pipeline with:\n",
        "- **Embedding** docs & storing them in **Azure AI Search**.\n",
        "- **Retrieving** top docs for user question.\n",
        "- **Chat** with the retrieved docs.\n",
        "\n",
        "🔎 You can expand this by adding advanced chunking, more robust retrieval, and quality checks. Enjoy your healthy coding! 🍎\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
