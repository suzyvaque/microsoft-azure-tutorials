{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Smart Resume Matching**\n",
        "### **Azure SQL DB 기반 RAG를 통한 유사성 검색**\n",
        "\n",
        "-----\n",
        "\n",
        "*\n",
        "\n",
        "본 튜토리얼은 [Microsoft Dev-Blog: Smart Resume Matching with Azure SQL DB and Document Intelligence](https://devblogs.microsoft.com/azure-sql/smart-resume-matching-with-azure-sql-db-document-intelligence/)를 참고하여 한국어로 작성됐습니다.\n",
        "\n",
        "현재 Microsoft를 비롯한 여러 회사에는 특정 직무에 적합한 이력서를 추천받거나 쉽게 키워드 검색을 할 수 있는 Smart Resume 서비스가 도입돼 있습니다. 본 튜토리얼은 Azure의 여러 서비스를 활용해 Smart Resume Matching을 직접 구현하는 것을 목표로 합니다. 이를 위해 Azure AI Services 중 Document Intelligence를 통한 PDF chunking, Azure OpenAI를 이용한 텍스트 임베딩, Azure SQL DB의 vector data type을 이용한 임베딩 저장 및 vector 유사도 검색과 RAG를 수행합니다.\n",
        "\n",
        "**기존 블로그의 내용에 더해, Azure OpenAI를 통해 Sample Dataset을 형성하는 과정과 API Management (APIM) 리소스를 도입해 로드밸런싱을 하는 과정을 추가했습니다.**\n",
        "\n",
        "*\n",
        "\n",
        "-----\n",
        "\n",
        "### Tutorial Overview\n",
        "\n",
        "다음의 과정을 따라 Smart Resume Matching을 구현합니다.\n",
        "\n",
        "1. Resume PDF Sample Data 생성 (with Azure OpenAI chat completion model)\n",
        "2. PDF Chunking (with Azure Document Intelligence)\n",
        "3. 임베딩 생성 (with Azure OpenAI text embdding model)\n",
        "4. 벡터 저장 및 유사도 검색 (with Azure SQL DB built-in vector functions)\n",
        "5. RAG 기반 답변 생성 (with Azure OpenAI chat completion model)\n",
        "\n",
        "-----\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1. Azure Subscription\n",
        "2. Azure Resources: Azure Document Intelligence, Azure SQL Database, Azure OpenAI (Chat Completion Model, Embedding Model), Azure API Management\n",
        "3. .env: 아래의 셀을 실행해 작성할 수 있습니다.\n",
        "4. Python: 본 튜토리얼은 3.10.11 버전으로 테스트 됐습니다.\n",
        "5. Jupyter Notebook: 본 튜토리얼은 Azure Machine Learning Studio의 Notebook (Python 3.10 - SDK v2) 환경에서 작성 및 테스트됐습니다.\n",
        "\n",
        "**구체적인 튜토리얼: [Microsoft Korea 테크블로그 (TBU)] (link_tbu)**\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "\n",
        "APIM_URL = \"\"\n",
        "APIM_SUBSCRIPTION_KEY = \"\"\n",
        "APIM_API_VERSION = \"\"\n",
        "APIM_CHAT_DEPLOYMENT_NAME = \"\"\n",
        "APIM_EMBEDDING_DEPLOYMENT_NAME = \"\"\n",
        "\n",
        "AZUREDOCINTELLIGENCE_ENDPOINT = \"\"\n",
        "AZUREDOCINTELLIGENCE_API_KEY = \"\"\n",
        "\n",
        "# Use only one of the below. The one you are not using should be commented out.\n",
        "\n",
        "# For Entra ID Service Principle Authentication: Paste Connection String {ODBC (Includes Node.js) (Microsoft Entra password authentication)} and change 18 to 17\n",
        "ENTRA_CONNECTION_STRING= \"\"\n",
        "\n",
        "# For SQL Authentication: Paste Connection String {ODBC (Includes Node.js) (SQL authentication)} and change Dirver 18 to 17\n",
        "SQL_CONNECTION_STRING= \"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경설정"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup the python libraries required for this notebook\n",
        "import sys\n",
        "!{sys.executable} -m pip install -r requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: tiktoken in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.8.0)\r\nRequirement already satisfied: tokenizer in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.4.5)\r\nRequirement already satisfied: azure-ai-documentintelligence in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.0.0)\r\nRequirement already satisfied: azure-ai-formrecognizer in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.3.3)\r\nRequirement already satisfied: azure-identity in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.18.0)\r\nRequirement already satisfied: azure-core in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.31.0)\r\nRequirement already satisfied: azure-search-documents==11.6.0b3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (11.6.0b3)\r\nRequirement already satisfied: python-dotenv in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.0.1)\r\nRequirement already satisfied: openai in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.61.0)\r\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.23.5)\r\nRequirement already satisfied: pyodbc in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (5.2.0)\r\nRequirement already satisfied: num2words in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.5.14)\r\nRequirement already satisfied: matplotlib in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (3.9.2)\r\nRequirement already satisfied: plotly in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (6.0.0)\r\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.14.1)\r\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.5.2)\r\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (1.3.5)\r\nRequirement already satisfied: PrettyTable in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (3.14.0)\r\nRequirement already satisfied: nltk in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (3.9.1)\r\nRequirement already satisfied: reportlab in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (4.2.5)\r\nRequirement already satisfied: markdown2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (2.5.3)\r\nRequirement already satisfied: azure-common>=1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-search-documents==11.6.0b3->-r requirements.txt (line 7)) (1.1.28)\r\nRequirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-search-documents==11.6.0b3->-r requirements.txt (line 7)) (0.6.1)\r\nRequirement already satisfied: regex>=2022.1.18 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 1)) (2024.11.6)\r\nRequirement already satisfied: requests>=2.26.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 1)) (2.32.3)\r\nRequirement already satisfied: typing-extensions>=4.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-ai-documentintelligence->-r requirements.txt (line 3)) (4.12.2)\r\nRequirement already satisfied: msrest>=0.6.21 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-ai-formrecognizer->-r requirements.txt (line 4)) (0.7.1)\r\nRequirement already satisfied: cryptography>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-identity->-r requirements.txt (line 5)) (43.0.1)\r\nRequirement already satisfied: msal>=1.30.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-identity->-r requirements.txt (line 5)) (1.31.0)\r\nRequirement already satisfied: msal-extensions>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-identity->-r requirements.txt (line 5)) (1.2.0)\r\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core->-r requirements.txt (line 6)) (1.16.0)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (4.8.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (1.9.0)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (0.28.1)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (0.8.2)\r\nRequirement already satisfied: pydantic<3,>=1.9.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (2.10.6)\r\nRequirement already satisfied: sniffio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (1.3.1)\r\nRequirement already satisfied: tqdm>4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai->-r requirements.txt (line 9)) (4.66.5)\r\nRequirement already satisfied: docopt>=0.6.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from num2words->-r requirements.txt (line 12)) (0.6.2)\r\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.0)\r\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (4.53.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.7)\r\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (24.1)\r\nRequirement already satisfied: pillow>=8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (10.4.0)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (3.1.4)\r\nRequirement already satisfied: python-dateutil>=2.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (2.9.0)\r\nRequirement already satisfied: narwhals>=1.15.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from plotly->-r requirements.txt (line 14)) (1.24.2)\r\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 16)) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 16)) (3.5.0)\r\nRequirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 17)) (2024.2)\r\nRequirement already satisfied: wcwidth in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from PrettyTable->-r requirements.txt (line 19)) (0.2.13)\r\nRequirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 20)) (8.1.7)\r\nRequirement already satisfied: chardet in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from reportlab->-r requirements.txt (line 21)) (5.2.0)\r\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 9)) (1.2.2)\r\nRequirement already satisfied: idna>=2.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 9)) (3.10)\r\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity->-r requirements.txt (line 5)) (1.17.1)\r\nRequirement already satisfied: certifi in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 9)) (2024.8.30)\r\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 9)) (1.0.7)\r\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 9)) (0.14.0)\r\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r requirements.txt (line 5)) (2.9.0)\r\nRequirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from msal-extensions>=1.2.0->azure-identity->-r requirements.txt (line 5)) (2.10.1)\r\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer->-r requirements.txt (line 4)) (2.0.0)\r\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 9)) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.27.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 9)) (2.27.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 1)) (3.3.2)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 1)) (2.2.3)\r\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->-r requirements.txt (line 5)) (2.22)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer->-r requirements.txt (line 4)) (3.2.2)\r\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "azdata_cell_guid": "fe37c601-5918-4055-badc-6c0ba90c68ce",
        "language": "python",
        "gather": {
          "logged": 1738730846639
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the env details\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "azdata_cell_guid": "4c29709e-1c3a-495d-83ec-05737e220847",
        "language": "python",
        "gather": {
          "logged": 1738737919208
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 0: AOAI를 이용한 Sample Data 생성**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "import markdown2"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1738737156640
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apim_url = os.getenv(\"APIM_URL\")\n",
        "deployment_name = os.getenv(\"APIM_CHAT_DEPLOYMENT_NAME\")\n",
        "api_version = os.getenv(\"APIM_API_VERSION\")\n",
        "subscription_key = os.getenv(\"APIM_SUBSCRIPTION_KEY\")\n",
        "\n",
        "# Construct the URL and headers\n",
        "\n",
        "chat_url = f\"{apim_url}/deployments/{deployment_name}/chat/completions?api-version={api_version}\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Ocp-Apim-Subscription-Key\": subscription_key\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1738734577938
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_resumes_dir = 'resumes'\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(folder_resumes_dir):\n",
        "    os.makedirs(folder_resumes_dir)\n",
        "    print(f\"Folder '{folder_resumes_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Folder '{folder_resumes_dir}' already exists.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Folder 'resumes' already exists.\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1738732086129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create n resumes as sample data\n",
        "\n",
        "n_resumes = 100\n",
        "\n",
        "for i in range(n_resumes):\n",
        "    json_payload = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a job seeker with some academic or work experience in exactly in one of the fields: software dev, embedded systems, data science, NLP, computer vision, or IT consulting. Create a half-page length resume with a random name, random degree, and work experiences only relevant to the previously selected field.\"\n",
        "            }\n",
        "        ],\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 800\n",
        "    }\n",
        "\n",
        "    response = requests.post(chat_url, headers=headers, json=json_payload)\n",
        "    result = response.json()['choices'][0]['message']['content']\n",
        "\n",
        "    # Save result as PDF\n",
        "    pdf_file = f'{folder_resumes_dir}/resume_{i}.pdf'\n",
        "    # Create a PDF file\n",
        "    doc = SimpleDocTemplate(pdf_file, pagesize=letter,\n",
        "                      rightMargin=20, leftMargin=20,\n",
        "                      topMargin=20, bottomMargin=20)\n",
        "    styles = getSampleStyleSheet()\n",
        "    styles.add(ParagraphStyle(name='Smaller', fontSize=7))\n",
        "    story = []\n",
        "\n",
        "    # Convert markdown to HTML\n",
        "    html_content = markdown2.markdown(result)\n",
        "\n",
        "    # Add the HTML content to the PDF\n",
        "    for line in html_content.split('\\n'):\n",
        "        story.append(Paragraph(line, styles['Smaller']))\n",
        "        story.append(Spacer(1, 2))\n",
        "        \n",
        "    doc.build(story)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1738731891005
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# **PART 1: Azure Document Intelligence를 이용한 PDF Extraction 및 Tokenization**\n",
        "-----\n",
        "Azure Document Intelligence를 사용하면 사전 정의된 모델로 문서를 분석할 수 있습니다. 본 튜토리얼에서 DocumentAnalysisClient는 그 중 [layout 모델](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/prebuilt/layout?view=doc-intel-4.0.0&tabs=rest%2Csample-code)을 사용해 이력서 PDF 파일에서 텍스트를 추출합니다. 텍스트는 tiktoken 라이브러리를 통해 인코딩돼 토큰들로 변환됩니다. 이때 하나의 단어는 하나 이상의 토큰으로 변환될 수 있습니다.\n",
        "\n",
        "다음 단계인 임베딩에서 텍스트가 Azure OpenAI 임베딩 모델의 처리 한도를 초과하지 않도록, 토큰들은 하나 이상의 chunk로 분할됩니다. Chunk는 500개 토큰 단위로 설정돼있습니다."
      ],
      "metadata": {
        "azdata_cell_guid": "4b543f05-9036-4887-8737-09aa9f865ec2",
        "language": "python"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "endpoint = os.getenv(\"AZUREDOCINTELLIGENCE_ENDPOINT\")\n",
        "api_key = os.getenv(\"AZUREDOCINTELLIGENCE_API_KEY\")\n",
        "\n",
        "# Create a DocumentAnalysisClient\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(api_key)\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "azdata_cell_guid": "b20cc66a-50ce-4486-b275-d4683f4ba545",
        "language": "python",
        "gather": {
          "logged": 1738737162100
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# folder_resumes_dir: Path to the directory containing PDF files, set earlier\n",
        "\n",
        "def get_pdf_files(folder_resumes_dir):\n",
        "    for path, subdirs, files in os.walk(folder_resumes_dir):\n",
        "        for name in files:\n",
        "            if (name.endswith(\".pdf\")):\n",
        "                yield os.path.join(path, name)\n",
        "\n",
        "# Function to read PDF files and extract text using Azure AI Document Intelligence\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", document=f)\n",
        "    result = poller.result()\n",
        "    text = \"\"\n",
        "    for page in result.pages:\n",
        "        for line in page.lines:\n",
        "            text += line.content + \" \"\n",
        "    return text\n",
        "\n",
        "# Function to clean text and remove special characters\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "# Function to split text into chunks of 500 tokens\n",
        "def split_text_into_token_chunks(text, max_tokens=500):\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    \n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk_tokens = tokens[i:i + max_tokens]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Count the number of PDF files in the directory\n",
        "pdf_files = [f for f in get_pdf_files(folder_resumes_dir)]\n",
        "num_files = len(pdf_files)\n",
        "print(f\"Number of PDF files in the directory: {num_files}\\n\")\n",
        "\n",
        "# Create a DataFrame to store the chunks\n",
        "data = []\n",
        "\n",
        "for file_id, pdf_file in enumerate(pdf_files):\n",
        "    file_name = os.path.basename(pdf_file)\n",
        "\n",
        "    pdf_path = pdf_file\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    cleaned_text = clean_text(text)\n",
        "    chunks = split_text_into_token_chunks(cleaned_text)\n",
        "    \n",
        "    print(f\"{file_name}... {len(chunks)} chunks\")\n",
        "    \n",
        "    for chunk_id, chunk in enumerate(chunks):\n",
        "        chunk_text = chunk.strip() if chunk.strip() else \"NULL\"\n",
        "        unique_chunk_id = f\"{file_id}_{chunk_id}\"\n",
        "        print(f\"{file_name}... Chunk ID: {chunk_id}, Unique Chunk ID: {unique_chunk_id}, Chunk Length: {len(chunk_text)}, Chunk Text: {chunk_text[:20]}...\\n\")  # Print first 20 characters of chunk text\n",
        "        data.append({\n",
        "            \"file_name\": file_name,\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"chunk_text\": chunk_text,\n",
        "            \"unique_chunk_id\": unique_chunk_id\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head(3)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of PDF files in the directory: 100\n\nresume_0.pdf... 1 chunks\nresume_0.pdf... Chunk ID: 0, Unique Chunk ID: 0_0, Chunk Length: 1890, Chunk Text: John Doe Education B...\n\nresume_1.pdf... 2 chunks\nresume_1.pdf... Chunk ID: 0, Unique Chunk ID: 1_0, Chunk Length: 2985, Chunk Text: Daniel Thompson dsth...\n\nresume_1.pdf... Chunk ID: 1, Unique Chunk ID: 1_1, Chunk Length: 95, Chunk Text: 2020  Present  Techn...\n\nresume_10.pdf... 1 chunks\nresume_10.pdf... Chunk ID: 0, Unique Chunk ID: 2_0, Chunk Length: 2589, Chunk Text: John Smith Email Add...\n\nresume_11.pdf... 1 chunks\nresume_11.pdf... Chunk ID: 0, Unique Chunk ID: 3_0, Chunk Length: 3054, Chunk Text: Jane Doe New York NY...\n\nresume_12.pdf... 1 chunks\nresume_12.pdf... Chunk ID: 0, Unique Chunk ID: 4_0, Chunk Length: 1794, Chunk Text: John Doe johndoeexam...\n\nresume_13.pdf... 1 chunks\nresume_13.pdf... Chunk ID: 0, Unique Chunk ID: 5_0, Chunk Length: 2609, Chunk Text: Ella Johnson Email e...\n\nresume_14.pdf... 1 chunks\nresume_14.pdf... Chunk ID: 0, Unique Chunk ID: 6_0, Chunk Length: 2533, Chunk Text: Name Claire Johnson ...\n\nresume_15.pdf... 1 chunks\nresume_15.pdf... Chunk ID: 0, Unique Chunk ID: 7_0, Chunk Length: 2029, Chunk Text: John Doe 1234 Elm St...\n\nresume_16.pdf... 1 chunks\nresume_16.pdf... Chunk ID: 0, Unique Chunk ID: 8_0, Chunk Length: 2792, Chunk Text: John Doe Contact Inf...\n\nresume_17.pdf... 1 chunks\nresume_17.pdf... Chunk ID: 0, Unique Chunk ID: 9_0, Chunk Length: 2184, Chunk Text: Sam Richardson Conta...\n\nresume_18.pdf... 1 chunks\nresume_18.pdf... Chunk ID: 0, Unique Chunk ID: 10_0, Chunk Length: 2075, Chunk Text: Christina Taylor Con...\n\nresume_19.pdf... 1 chunks\nresume_19.pdf... Chunk ID: 0, Unique Chunk ID: 11_0, Chunk Length: 2582, Chunk Text: Akira Tanaka Contact...\n\nresume_2.pdf... 1 chunks\nresume_2.pdf... Chunk ID: 0, Unique Chunk ID: 12_0, Chunk Length: 2706, Chunk Text: John Doe 1234 Elm St...\n\nresume_20.pdf... 1 chunks\nresume_20.pdf... Chunk ID: 0, Unique Chunk ID: 13_0, Chunk Length: 2263, Chunk Text: Name Johnathan Doe E...\n\nresume_21.pdf... 1 chunks\nresume_21.pdf... Chunk ID: 0, Unique Chunk ID: 14_0, Chunk Length: 2609, Chunk Text: Alexander Johnson al...\n\nresume_22.pdf... 1 chunks\nresume_22.pdf... Chunk ID: 0, Unique Chunk ID: 15_0, Chunk Length: 2555, Chunk Text: James Reynolds Conta...\n\nresume_23.pdf... 1 chunks\nresume_23.pdf... Chunk ID: 0, Unique Chunk ID: 16_0, Chunk Length: 2500, Chunk Text: Johnathan Smith Emai...\n\nresume_24.pdf... 1 chunks\nresume_24.pdf... Chunk ID: 0, Unique Chunk ID: 17_0, Chunk Length: 3027, Chunk Text: Johnathan T Smith Lo...\n\nresume_25.pdf... 1 chunks\nresume_25.pdf... Chunk ID: 0, Unique Chunk ID: 18_0, Chunk Length: 2478, Chunk Text: John Doe Contact Inf...\n\nresume_26.pdf... 1 chunks\nresume_26.pdf... Chunk ID: 0, Unique Chunk ID: 19_0, Chunk Length: 2760, Chunk Text: John Smith Email joh...\n\nresume_27.pdf... 1 chunks\nresume_27.pdf... Chunk ID: 0, Unique Chunk ID: 20_0, Chunk Length: 2432, Chunk Text: John L Smith Contact...\n\nresume_28.pdf... 1 chunks\nresume_28.pdf... Chunk ID: 0, Unique Chunk ID: 21_0, Chunk Length: 2696, Chunk Text: Alex Thompson Contac...\n\nresume_29.pdf... 1 chunks\nresume_29.pdf... Chunk ID: 0, Unique Chunk ID: 22_0, Chunk Length: 2678, Chunk Text: John A Doe Email joh...\n\nresume_3.pdf... 1 chunks\nresume_3.pdf... Chunk ID: 0, Unique Chunk ID: 23_0, Chunk Length: 2677, Chunk Text: John Doe Phone 555 5...\n\nresume_30.pdf... 1 chunks\nresume_30.pdf... Chunk ID: 0, Unique Chunk ID: 24_0, Chunk Length: 2576, Chunk Text: Jane Doe Email janed...\n\nresume_31.pdf... 1 chunks\nresume_31.pdf... Chunk ID: 0, Unique Chunk ID: 25_0, Chunk Length: 2044, Chunk Text: Avery Johnson Email ...\n\nresume_32.pdf... 1 chunks\nresume_32.pdf... Chunk ID: 0, Unique Chunk ID: 26_0, Chunk Length: 2223, Chunk Text: Name Jordan Mitchell...\n\nresume_33.pdf... 1 chunks\nresume_33.pdf... Chunk ID: 0, Unique Chunk ID: 27_0, Chunk Length: 2120, Chunk Text: Resume John Doe Emai...\n\nresume_34.pdf... 1 chunks\nresume_34.pdf... Chunk ID: 0, Unique Chunk ID: 28_0, Chunk Length: 2243, Chunk Text: John Smith Email joh...\n\nresume_35.pdf... 1 chunks\nresume_35.pdf... Chunk ID: 0, Unique Chunk ID: 29_0, Chunk Length: 1938, Chunk Text: John Doe Email johnd...\n\nresume_36.pdf... 1 chunks\nresume_36.pdf... Chunk ID: 0, Unique Chunk ID: 30_0, Chunk Length: 1787, Chunk Text: Jonas Wright Contact...\n\nresume_37.pdf... 1 chunks\nresume_37.pdf... Chunk ID: 0, Unique Chunk ID: 31_0, Chunk Length: 2429, Chunk Text: Alexander Reynolds E...\n\nresume_38.pdf... 1 chunks\nresume_38.pdf... Chunk ID: 0, Unique Chunk ID: 32_0, Chunk Length: 2834, Chunk Text: Jane Doe Email janed...\n\nresume_39.pdf... 1 chunks\nresume_39.pdf... Chunk ID: 0, Unique Chunk ID: 33_0, Chunk Length: 2700, Chunk Text: Name Alex Smith Emai...\n\nresume_4.pdf... 1 chunks\nresume_4.pdf... Chunk ID: 0, Unique Chunk ID: 34_0, Chunk Length: 2658, Chunk Text: Alex Johnson Email a...\n\nresume_40.pdf... 1 chunks\nresume_40.pdf... Chunk ID: 0, Unique Chunk ID: 35_0, Chunk Length: 2502, Chunk Text: Adam Mercer adammerc...\n\nresume_41.pdf... 1 chunks\nresume_41.pdf... Chunk ID: 0, Unique Chunk ID: 36_0, Chunk Length: 2153, Chunk Text: John Doe Email johnd...\n\nresume_42.pdf... 1 chunks\nresume_42.pdf... Chunk ID: 0, Unique Chunk ID: 37_0, Chunk Length: 2289, Chunk Text: James A Walker Phone...\n\nresume_43.pdf... 1 chunks\nresume_43.pdf... Chunk ID: 0, Unique Chunk ID: 38_0, Chunk Length: 2667, Chunk Text: Name Alex Johnson De...\n\nresume_44.pdf... 1 chunks\nresume_44.pdf... Chunk ID: 0, Unique Chunk ID: 39_0, Chunk Length: 2823, Chunk Text: Johnathan Carter Ema...\n\nresume_45.pdf... 1 chunks\nresume_45.pdf... Chunk ID: 0, Unique Chunk ID: 40_0, Chunk Length: 2635, Chunk Text: Resume Name Jane Smi...\n\nresume_46.pdf... 1 chunks\nresume_46.pdf... Chunk ID: 0, Unique Chunk ID: 41_0, Chunk Length: 2625, Chunk Text: Johnathan Blake Emai...\n\nresume_47.pdf... 1 chunks\nresume_47.pdf... Chunk ID: 0, Unique Chunk ID: 42_0, Chunk Length: 2593, Chunk Text: Robert Johnson Email...\n\nresume_48.pdf... 1 chunks\nresume_48.pdf... Chunk ID: 0, Unique Chunk ID: 43_0, Chunk Length: 2728, Chunk Text: Michael Anderson Con...\n\nresume_49.pdf... 1 chunks\nresume_49.pdf... Chunk ID: 0, Unique Chunk ID: 44_0, Chunk Length: 2785, Chunk Text: Resume Name Alex Tho...\n\nresume_5.pdf... 1 chunks\nresume_5.pdf... Chunk ID: 0, Unique Chunk ID: 45_0, Chunk Length: 2577, Chunk Text: John Doe 123 Main St...\n\nresume_50.pdf... 1 chunks\nresume_50.pdf... Chunk ID: 0, Unique Chunk ID: 46_0, Chunk Length: 2583, Chunk Text: John Doe Email johnd...\n\nresume_51.pdf... 1 chunks\nresume_51.pdf... Chunk ID: 0, Unique Chunk ID: 47_0, Chunk Length: 2676, Chunk Text: Your Name Your Addre...\n\nresume_52.pdf... 1 chunks\nresume_52.pdf... Chunk ID: 0, Unique Chunk ID: 48_0, Chunk Length: 2662, Chunk Text: John Doe 1234 Elm St...\n\nresume_53.pdf... 1 chunks\nresume_53.pdf... Chunk ID: 0, Unique Chunk ID: 49_0, Chunk Length: 2493, Chunk Text: Alex Johnson Email a...\n\nresume_54.pdf... 1 chunks\nresume_54.pdf... Chunk ID: 0, Unique Chunk ID: 50_0, Chunk Length: 2787, Chunk Text: John D Mitchell Cont...\n\nresume_55.pdf... 1 chunks\nresume_55.pdf... Chunk ID: 0, Unique Chunk ID: 51_0, Chunk Length: 2749, Chunk Text: Alex Johnson Email a...\n\nresume_56.pdf... 1 chunks\nresume_56.pdf... Chunk ID: 0, Unique Chunk ID: 52_0, Chunk Length: 2140, Chunk Text: John Doe Contact Inf...\n\nresume_57.pdf... 1 chunks\nresume_57.pdf... Chunk ID: 0, Unique Chunk ID: 53_0, Chunk Length: 2850, Chunk Text: John Doe Contact Inf...\n\nresume_58.pdf... 1 chunks\nresume_58.pdf... Chunk ID: 0, Unique Chunk ID: 54_0, Chunk Length: 2017, Chunk Text: Alex Johnson Contact...\n\nresume_59.pdf... 1 chunks\nresume_59.pdf... Chunk ID: 0, Unique Chunk ID: 55_0, Chunk Length: 2052, Chunk Text: John Doe johndoegmai...\n\nresume_6.pdf... 1 chunks\nresume_6.pdf... Chunk ID: 0, Unique Chunk ID: 56_0, Chunk Length: 2537, Chunk Text: John Doe 123 Tech La...\n\nresume_60.pdf... 1 chunks\nresume_60.pdf... Chunk ID: 0, Unique Chunk ID: 57_0, Chunk Length: 2165, Chunk Text: Alex Smith Email ale...\n\nresume_61.pdf... 1 chunks\nresume_61.pdf... Chunk ID: 0, Unique Chunk ID: 58_0, Chunk Length: 2689, Chunk Text: Name Ava Montoya Con...\n\nresume_62.pdf... 1 chunks\nresume_62.pdf... Chunk ID: 0, Unique Chunk ID: 59_0, Chunk Length: 2623, Chunk Text: Alex Johnson Email a...\n\nresume_63.pdf... 1 chunks\nresume_63.pdf... Chunk ID: 0, Unique Chunk ID: 60_0, Chunk Length: 2412, Chunk Text: Alex Johnson Email a...\n\nresume_64.pdf... 1 chunks\nresume_64.pdf... Chunk ID: 0, Unique Chunk ID: 61_0, Chunk Length: 2406, Chunk Text: Name Claire Thompson...\n\nresume_65.pdf... 1 chunks\nresume_65.pdf... Chunk ID: 0, Unique Chunk ID: 62_0, Chunk Length: 2175, Chunk Text: Emma Johnson Contact...\n\nresume_66.pdf... 1 chunks\nresume_66.pdf... Chunk ID: 0, Unique Chunk ID: 63_0, Chunk Length: 2530, Chunk Text: John Smith Location ...\n\nresume_67.pdf... 1 chunks\nresume_67.pdf... Chunk ID: 0, Unique Chunk ID: 64_0, Chunk Length: 2753, Chunk Text: Alex Johnson alexjoh...\n\nresume_68.pdf... 1 chunks\nresume_68.pdf... Chunk ID: 0, Unique Chunk ID: 65_0, Chunk Length: 2535, Chunk Text: JAMES ANDERSON 1234 ...\n\nresume_69.pdf... 1 chunks\nresume_69.pdf... Chunk ID: 0, Unique Chunk ID: 66_0, Chunk Length: 1721, Chunk Text: John Doe Software De...\n\nresume_7.pdf... 1 chunks\nresume_7.pdf... Chunk ID: 0, Unique Chunk ID: 67_0, Chunk Length: 2020, Chunk Text: Laura Spencer Email ...\n\nresume_70.pdf... 1 chunks\nresume_70.pdf... Chunk ID: 0, Unique Chunk ID: 68_0, Chunk Length: 2021, Chunk Text: John Doe Email johnd...\n\nresume_71.pdf... 1 chunks\nresume_71.pdf... Chunk ID: 0, Unique Chunk ID: 69_0, Chunk Length: 2345, Chunk Text: Alex Thompson Contac...\n\nresume_72.pdf... 1 chunks\nresume_72.pdf... Chunk ID: 0, Unique Chunk ID: 70_0, Chunk Length: 2709, Chunk Text: Name Jordan Mitchell...\n\nresume_73.pdf... 1 chunks\nresume_73.pdf... Chunk ID: 0, Unique Chunk ID: 71_0, Chunk Length: 2023, Chunk Text: Alice Smith Contact ...\n\nresume_74.pdf... 1 chunks\nresume_74.pdf... Chunk ID: 0, Unique Chunk ID: 72_0, Chunk Length: 2276, Chunk Text: Resume Name Jane Doe...\n\nresume_75.pdf... 1 chunks\nresume_75.pdf... Chunk ID: 0, Unique Chunk ID: 73_0, Chunk Length: 2679, Chunk Text: Name John Doe Contac...\n\nresume_76.pdf... 2 chunks\nresume_76.pdf... Chunk ID: 0, Unique Chunk ID: 74_0, Chunk Length: 2986, Chunk Text: Alex Johnson Email a...\n\nresume_76.pdf... Chunk ID: 1, Unique Chunk ID: 74_1, Chunk Length: 53, Chunk Text: Johnson Email alexjo...\n\nresume_77.pdf... 1 chunks\nresume_77.pdf... Chunk ID: 0, Unique Chunk ID: 75_0, Chunk Length: 2474, Chunk Text: Name Johnathan Orteg...\n\nresume_78.pdf... 1 chunks\nresume_78.pdf... Chunk ID: 0, Unique Chunk ID: 76_0, Chunk Length: 2914, Chunk Text: John D Harrison Emai...\n\nresume_79.pdf... 1 chunks\nresume_79.pdf... Chunk ID: 0, Unique Chunk ID: 77_0, Chunk Length: 2774, Chunk Text: Cameron Reynolds Con...\n\nresume_8.pdf... 2 chunks\nresume_8.pdf... Chunk ID: 0, Unique Chunk ID: 78_0, Chunk Length: 2954, Chunk Text: John Doe Phone 555 1...\n\nresume_8.pdf... Chunk ID: 1, Unique Chunk ID: 78_1, Chunk Length: 210, Chunk Text: NLP model to assess ...\n\nresume_80.pdf... 1 chunks\nresume_80.pdf... Chunk ID: 0, Unique Chunk ID: 79_0, Chunk Length: 2357, Chunk Text: Name Jordan Smith Ad...\n\nresume_81.pdf... 1 chunks\nresume_81.pdf... Chunk ID: 0, Unique Chunk ID: 80_0, Chunk Length: 2274, Chunk Text: Name Jane Doe Email ...\n\nresume_82.pdf... 1 chunks\nresume_82.pdf... Chunk ID: 0, Unique Chunk ID: 81_0, Chunk Length: 2447, Chunk Text: John Doe Contact Ema...\n\nresume_83.pdf... 1 chunks\nresume_83.pdf... Chunk ID: 0, Unique Chunk ID: 82_0, Chunk Length: 2248, Chunk Text: John Doe Email johnd...\n\nresume_84.pdf... 2 chunks\nresume_84.pdf... Chunk ID: 0, Unique Chunk ID: 83_0, Chunk Length: 2951, Chunk Text: Samuel Clark 1234 In...\n\nresume_84.pdf... Chunk ID: 1, Unique Chunk ID: 83_1, Chunk Length: 46, Chunk Text: contribute to the su...\n\nresume_85.pdf... 1 chunks\nresume_85.pdf... Chunk ID: 0, Unique Chunk ID: 84_0, Chunk Length: 2704, Chunk Text: Sarah Johnson Email ...\n\nresume_86.pdf... 1 chunks\nresume_86.pdf... Chunk ID: 0, Unique Chunk ID: 85_0, Chunk Length: 3147, Chunk Text: Kevin J Miles Email ...\n\nresume_87.pdf... 1 chunks\nresume_87.pdf... Chunk ID: 0, Unique Chunk ID: 86_0, Chunk Length: 2937, Chunk Text: Jane Doe Embedded Sy...\n\nresume_88.pdf... 1 chunks\nresume_88.pdf... Chunk ID: 0, Unique Chunk ID: 87_0, Chunk Length: 2634, Chunk Text: John Doe 123 Technol...\n\nresume_89.pdf... 1 chunks\nresume_89.pdf... Chunk ID: 0, Unique Chunk ID: 88_0, Chunk Length: 2987, Chunk Text: John Doe johndoeemai...\n\nresume_9.pdf... 1 chunks\nresume_9.pdf... Chunk ID: 0, Unique Chunk ID: 89_0, Chunk Length: 2128, Chunk Text: John Doe Email johnd...\n\nresume_90.pdf... 1 chunks\nresume_90.pdf... Chunk ID: 0, Unique Chunk ID: 90_0, Chunk Length: 2068, Chunk Text: John Smith Email joh...\n\nresume_91.pdf... 1 chunks\nresume_91.pdf... Chunk ID: 0, Unique Chunk ID: 91_0, Chunk Length: 2418, Chunk Text: Lisa Thompson Contac...\n\nresume_92.pdf... 1 chunks\nresume_92.pdf... Chunk ID: 0, Unique Chunk ID: 92_0, Chunk Length: 2371, Chunk Text: Emma Johnson Email e...\n\nresume_93.pdf... 1 chunks\nresume_93.pdf... Chunk ID: 0, Unique Chunk ID: 93_0, Chunk Length: 3105, Chunk Text: Resume Name John Smi...\n\nresume_94.pdf... 1 chunks\nresume_94.pdf... Chunk ID: 0, Unique Chunk ID: 94_0, Chunk Length: 2090, Chunk Text: John Doe Email johnd...\n\nresume_95.pdf... 1 chunks\nresume_95.pdf... Chunk ID: 0, Unique Chunk ID: 95_0, Chunk Length: 2108, Chunk Text: John Doe Email johnd...\n\nresume_96.pdf... 1 chunks\nresume_96.pdf... Chunk ID: 0, Unique Chunk ID: 96_0, Chunk Length: 2462, Chunk Text: John Doe Email johnd...\n\nresume_97.pdf... 1 chunks\nresume_97.pdf... Chunk ID: 0, Unique Chunk ID: 97_0, Chunk Length: 2196, Chunk Text: John Doe Email Addre...\n\nresume_98.pdf... 1 chunks\nresume_98.pdf... Chunk ID: 0, Unique Chunk ID: 98_0, Chunk Length: 2832, Chunk Text: Johnathan Doe Email ...\n\nresume_99.pdf... 1 chunks\nresume_99.pdf... Chunk ID: 0, Unique Chunk ID: 99_0, Chunk Length: 2655, Chunk Text: Resume Name Chris OC...\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "      file_name  chunk_id                                         chunk_text  \\\n0  resume_0.pdf         0  John Doe Education Bachelor of Science in Comp...   \n1  resume_1.pdf         0  Daniel Thompson dsthompsonexamplecom  123 4567...   \n2  resume_1.pdf         1  2020  Present  Technical Volunteer DataKind NY...   \n\n  unique_chunk_id  \n0             0_0  \n1             1_0  \n2             1_1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>chunk_id</th>\n      <th>chunk_text</th>\n      <th>unique_chunk_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>resume_0.pdf</td>\n      <td>0</td>\n      <td>John Doe Education Bachelor of Science in Comp...</td>\n      <td>0_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>resume_1.pdf</td>\n      <td>0</td>\n      <td>Daniel Thompson dsthompsonexamplecom  123 4567...</td>\n      <td>1_0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>resume_1.pdf</td>\n      <td>1</td>\n      <td>2020  Present  Technical Volunteer DataKind NY...</td>\n      <td>1_1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "azdata_cell_guid": "0355f92c-0546-4eac-aea8-b55d8bbef194",
        "language": "python",
        "gather": {
          "logged": 1738737617881
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "(Optional) 아래의 셀을 실행해 기존의 단어 개수와 최종 토큰 개수를 비교해봅시다."
      ],
      "metadata": {
        "azdata_cell_guid": "655a8389-1018-4e80-a39d-85187cf3c46f",
        "language": "python"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare number of tokens to number of words\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "sample_encode = tokenizer.encode(df.chunk_text[0])\n",
        "words = df.chunk_text[0].count(\" \")+1\n",
        "decode = tokenizer.decode_tokens_bytes(sample_encode)\n",
        "decoded_result = ', '.join([token.decode('utf-8') for token in decode])\n",
        "\n",
        "print(df.chunk_text[0])\n",
        "print(f\"\\nOriginally {words} words\\nMapped to {len(decode)} tokens\\n\")\n",
        "print(decoded_result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "John Doe Education Bachelor of Science in Computer Science University of California Berkeley Graduation May 2023 Experience Software Engineer Intern Tech Solutions Inc  San Francisco CA June 2022  August 2022  Developed and optimized frontend features using React reducing page load time by 25 and improving user experience  Collaborated with backend engineers to integrate APIs ensuring seamless data retrieval and user interaction  Conducted code reviews and unit testing to maintain code quality and reliability Junior Software Developer Innovex Systems  Los Angeles CA September 2021  May 2022  Participated in the full software development lifecycle for a web application aimed at enhancing digital collaboration  Implemented RESTful APIs using Nodejs improving data handling efficiency  Identified and fixed bugs in existing software resulting in a 15 increase in application stability Programming Tutor University of California Berkeley  Berkeley CA September 2019  May 2023  Provided oneonone tutoring sessions for students in introductory and advanced programming courses  Assisted students in developing problemsolving skills and understanding complex computing concepts  Created educational materials to aid in the comprehension of algorithms and data structures Skills  Programming Languages JavaScript Python Java C  Frameworks  Libraries React Nodejs Express  Tools  Platforms Git Docker Jenkins  Database Systems MySQL MongoDB  Web Development HTML CSS API Integration Projects  Developed a fullstack web application using the MERN stack MongoDB Expressjs React Nodejs for a collaborative project management tool  Implemented a machine learning model in Python to predict housing prices utilizing data preprocessing and feature engineering to achieve accurate predictions Contact Email johndoeexamplecom LinkedIn linkedincominjohndoe GitHub githubcomjohndoe Phone 123 4567890\n\nOriginally 275 words\nDecoded to 329 tokens\n\nJohn,  Doe,  Education,  Bachelor,  of,  Science,  in,  Computer,  Science,  University,  of,  California,  Berkeley,  Grad, uation,  May,  , 202, 3,  Experience,  Software,  Engineer,  Intern,  Tech,  Solutions,  Inc,  ,  San,  Francisco,  CA,  June,  , 202, 2,  ,  August,  , 202, 2,  ,  Developed,  and,  optimized,  frontend,  features,  using,  React,  reducing,  page,  load,  time,  by,  , 25,  and,  improving,  user,  experience,  ,  Collabor, ated,  with,  backend,  engineers,  to,  integrate,  APIs,  ensuring,  seamless,  data,  retrieval,  and,  user,  interaction,  ,  Conduct, ed,  code,  reviews,  and,  unit,  testing,  to,  maintain,  code,  quality,  and,  reliability,  Junior,  Software,  Developer,  Innov, ex,  Systems,  ,  Los,  Angeles,  CA,  September,  , 202, 1,  ,  May,  , 202, 2,  ,  Particip, ated,  in,  the,  full,  software,  development,  lifecycle,  for,  a,  web,  application,  aimed,  at,  enhancing,  digital,  collaboration,  ,  Implemented,  REST, ful,  APIs,  using,  Node, js,  improving,  data,  handling,  efficiency,  ,  Identified,  and,  fixed,  bugs,  in,  existing,  software,  resulting,  in,  a,  , 15,  increase,  in,  application,  stability,  Programming,  Tutor,  University,  of,  California,  Berkeley,  ,  Berkeley,  CA,  September,  , 201, 9,  ,  May,  , 202, 3,  ,  Provided,  one, on, one,  tutoring,  sessions,  for,  students,  in,  introductory,  and,  advanced,  programming,  courses,  ,  Ass, isted,  students,  in,  developing,  problems, olving,  skills,  and,  understanding,  complex,  computing,  concepts,  ,  Created,  educational,  materials,  to,  aid,  in,  the,  comprehension,  of,  algorithms,  and,  data,  structures,  Skills,  ,  Programming,  Languages,  JavaScript,  Python,  Java,  C,  ,  Framework, s,  ,  Libraries,  React,  Node, js,  Express,  ,  Tools,  ,  Platforms,  Git,  Docker,  Jenkins,  ,  Database,  Systems,  MySQL,  MongoDB,  ,  Web,  Development,  HTML,  CSS,  API,  Integration,  Projects,  ,  Developed,  a,  full, stack,  web,  application,  using,  the,  M, ERN,  stack,  MongoDB,  Express, js,  React,  Node, js,  for,  a,  collaborative,  project,  management,  tool,  ,  Implemented,  a,  machine,  learning,  model,  in,  Python,  to,  predict,  housing,  prices,  utilizing,  data,  preprocessing,  and,  feature,  engineering,  to,  achieve,  accurate,  predictions,  Contact,  Email,  j, oh, nd, oe, example, com,  LinkedIn,  linked, in, com, in, j, oh, nd, oe,  GitHub,  github, com, j, oh, nd, oe,  Phone,  , 123,  , 456, 789, 0\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "azdata_cell_guid": "28188280-2bef-414a-a663-a017c944bc19",
        "language": "python",
        "gather": {
          "logged": 1738737618834
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# **PART 2 : Azure Open AI를 이용한 임베딩 생성**\n",
        "\n",
        "토큰화의 결과인 각 chunk에 대한 임베딩을 생성하는 과정입니다.\n",
        "\n",
        "임베딩은 단어의 의미 관계를 고려한 값으로, 이후 유사도 검색을 할 수 있는 기반이 됩니다."
      ],
      "metadata": {
        "azdata_cell_guid": "ac19ee49-763a-4e2e-8c31-9ce1ba77bbe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from num2words import num2words\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import AzureOpenAI"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "apim_url = os.getenv(\"APIM_URL\")\n",
        "deployment_name = os.getenv(\"APIM_EMBEDDING_DEPLOYMENT_NAME\")\n",
        "api_version = os.getenv(\"APIM_API_VERSION\")\n",
        "subscription_key = os.getenv(\"APIM_SUBSCRIPTION_KEY\")\n",
        "\n",
        "# Construct the URL and headers\n",
        "\n",
        "embedding_url = f\"{apim_url}/deployments/{deployment_name}/embeddings?api-version={api_version}\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Ocp-Apim-Subscription-Key\": subscription_key\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Get sentence embedding using the Azure OpenAI text-embedding-small model.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to embed.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.post(embedding_url, headers=headers, json={\"input\": [text]})\n",
        "    # Embed the extracted chunk as text\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        embedding = json.loads(str(response_json['data'][0]['embedding']))\n",
        "        return embedding\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "all_filenames = []\n",
        "all_chunkids = []\n",
        "all_chunks = []\n",
        "all_embeddings = []\n",
        "\n",
        "# Assuming df is already defined with the required columns\n",
        "for index, row in df.iterrows():\n",
        "    filename = row['file_name']\n",
        "    chunkid = row['unique_chunk_id']\n",
        "    chunk = row['chunk_text']\n",
        "    embedding = get_embedding(chunk)\n",
        "    \n",
        "    if embedding is not None:\n",
        "        all_filenames.append(filename)\n",
        "        all_chunkids.append(chunkid)\n",
        "        all_chunks.append(chunk)\n",
        "        all_embeddings.append(embedding)\n",
        "    \n",
        "    if (index + 1) % 50 == 0:  # Print progress every 50 rows\n",
        "        print(f\"Completed {index + 1} rows\")\n",
        "\n",
        "# Create a new DataFrame with the results\n",
        "result_df = pd.DataFrame({\n",
        "    'filename': all_filenames,\n",
        "    'chunkid': all_chunkids,\n",
        "    'chunk': all_chunks,\n",
        "    'embedding': all_embeddings\n",
        "})\n",
        "\n",
        "result_df.head(5)  # Display the first few rows of the dataframe\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Completed 50 rows\nCompleted 100 rows\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "        filename chunkid                                              chunk  \\\n0   resume_0.pdf     0_0  John Doe Education Bachelor of Science in Comp...   \n1   resume_1.pdf     1_0  Daniel Thompson dsthompsonexamplecom  123 4567...   \n2   resume_1.pdf     1_1  2020  Present  Technical Volunteer DataKind NY...   \n3  resume_10.pdf     2_0  John Smith Email Address  Phone Number  Linked...   \n4  resume_11.pdf     3_0  Jane Doe New York NY Email janedoeexamplecom P...   \n\n                                           embedding  \n0  [-0.011703418, -0.013349316, 0.0018044173, -0....  \n1  [-0.048026104, 0.00015640476, 0.03146353, -0.0...  \n2  [-0.018442072, -0.034961402, 0.0030465978, -0....  \n3  [-0.0012448077, -0.00021796567, -0.004634464, ...  \n4  [-0.022091128, -0.0047286567, 0.015094948, -0....  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>chunkid</th>\n      <th>chunk</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>resume_0.pdf</td>\n      <td>0_0</td>\n      <td>John Doe Education Bachelor of Science in Comp...</td>\n      <td>[-0.011703418, -0.013349316, 0.0018044173, -0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>resume_1.pdf</td>\n      <td>1_0</td>\n      <td>Daniel Thompson dsthompsonexamplecom  123 4567...</td>\n      <td>[-0.048026104, 0.00015640476, 0.03146353, -0.0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>resume_1.pdf</td>\n      <td>1_1</td>\n      <td>2020  Present  Technical Volunteer DataKind NY...</td>\n      <td>[-0.018442072, -0.034961402, 0.0030465978, -0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>resume_10.pdf</td>\n      <td>2_0</td>\n      <td>John Smith Email Address  Phone Number  Linked...</td>\n      <td>[-0.0012448077, -0.00021796567, -0.004634464, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>resume_11.pdf</td>\n      <td>3_0</td>\n      <td>Jane Doe New York NY Email janedoeexamplecom P...</td>\n      <td>[-0.022091128, -0.0047286567, 0.015094948, -0....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "azdata_cell_guid": "3aeda057-d0c0-40cd-bdcf-723abfed94e2",
        "language": "python",
        "gather": {
          "logged": 1738737728895
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# **PART 3 : Azure SQL DB를 사용한 vector 저장**"
      ],
      "metadata": {
        "azdata_cell_guid": "04d42351-41ec-4ce9-9778-fe4ea2ecb8a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SQL DB 연결**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyodbc\n",
        "import struct\n",
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "from azure.identity import DefaultAzureCredential"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#lets define a function to connect to SQLDB\n",
        "\n",
        "def get_mssql_connection():\n",
        "    # Retrieve the connection string from the environment variables\n",
        "    entra_connection_string = os.getenv('ENTRA_CONNECTION_STRING')\n",
        "    sql_connection_string = os.getenv('SQL_CONNECTION_STRING')\n",
        "    \n",
        "    # Determine the authentication method and connect to the database\n",
        "    if entra_connection_string:\n",
        "        # Entra ID Service Principal Authentication\n",
        "        credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)    \n",
        "        token = credential.get_token('https://database.windows.net/.default')\n",
        "        token_bytes = token.token.encode('UTF-16LE')\n",
        "        token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
        "        SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by Microsoft in msodbcsql.h\n",
        "        conn = pyodbc.connect(entra_connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
        "    \n",
        "    elif sql_connection_string:\n",
        "        # SQL Authentication\n",
        "        conn = pyodbc.connect(sql_connection_string)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(\"No valid connection string found in the environment variables.\")\n",
        "\n",
        "    return conn"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "azdata_cell_guid": "930a63bc-4c08-4205-b152-b1ad5c82057a",
        "language": "python",
        "gather": {
          "logged": 1738737949420
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### **임베딩을 vector data type으로 저장하기**"
      ],
      "metadata": {
        "azdata_cell_guid": "e929c112-65d4-46f1-a9a2-7c9434b2d7cb",
        "language": "python"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the connection string from the function get_mssql_connection()\n",
        "conn = get_mssql_connection()\n",
        "\n",
        "try:\n",
        "    print(f\"Success: {conn}\")\n",
        "except:\n",
        "    print(\"Failed sql connection.\")\n",
        "\n",
        "\n",
        "# Create a cursor object\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table\n",
        "create_table_query = \"\"\"\n",
        "IF OBJECT_ID('dbo.resumedocs', 'U') IS NOT NULL\n",
        "DROP TABLE dbo.resumedocs\n",
        "\n",
        "CREATE TABLE resumedocs (\n",
        "        chunkid NVARCHAR(MAX),\n",
        "        filename NVARCHAR(255),\n",
        "        chunk NVARCHAR(MAX),\n",
        "        embedding VECTOR(1536)\n",
        "    )\n",
        "\"\"\"\n",
        "cursor.execute(create_table_query)\n",
        "conn.commit()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Success: <pyodbc.Connection object at 0x7f1216fe1550>\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1738738166714
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable fast_executemany\n",
        "cursor.fast_executemany = True\n",
        "\n",
        "# Loop through the DataFrame rows and insert them into the table\n",
        "for index, row in result_df.iterrows():\n",
        "    chunkid = row['chunkid']\n",
        "    filename = row['filename']\n",
        "    chunk = row['chunk']\n",
        "    embedding = row['embedding']\n",
        "    \n",
        "    # Use placeholders for the parameters in the SQL query\n",
        "    query = f\"\"\"\n",
        "    INSERT INTO resumedocs (chunkid, filename, chunk, embedding)\n",
        "    VALUES (?, ?, ?, CAST(CAST(? as NVARCHAR(MAX)) AS VECTOR(1536)))\n",
        "    \"\"\"\n",
        "    # Execute the query with the parameters\n",
        "    cursor.execute(query, chunkid, filename, chunk, json.dumps(embedding))\n",
        "\n",
        "# Commit the changes\n",
        "conn.commit()\n",
        "\n",
        "# Print a success message\n",
        "print(\"Data inserted successfully into the 'resumedocs' table.\")\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data inserted successfully into the 'resumedocs' table.\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "azdata_cell_guid": "680259d9-77ce-4b63-b412-9bea33bb0f43",
        "language": "python",
        "gather": {
          "logged": 1738738201887
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "생성되고 텍스트 임베딩이 저장된 SQL 테이블의 형태는 아래와 같습니다."
      ],
      "metadata": {
        "azdata_cell_guid": "ede638b9-d681-4cb9-9ab8-9651e3099a36",
        "language": "python"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the connection string from the environment variables\n",
        "conn = get_mssql_connection()\n",
        "\n",
        "# Create a cursor object\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Use placeholders for the parameters in the SQL query\n",
        "query = \"SELECT TOP(10) filename, chunkid, chunk, CAST(embedding AS NVARCHAR(MAX)) as embedding FROM dbo.resumedocs ORDER BY chunkid\"\n",
        "\n",
        "# Execute the query with the parameters\n",
        "cursor.execute(query)\n",
        "queryresults = cursor.fetchall()\n",
        "\n",
        "# Get column names from cursor.description\n",
        "column_names = [column[0] for column in cursor.description]\n",
        "\n",
        "# Create a PrettyTable object\n",
        "table = PrettyTable()\n",
        "\n",
        "# Add column names to the table\n",
        "table.field_names = column_names\n",
        "\n",
        "# Set max width for each column to truncate data\n",
        "table.max_width = 20\n",
        "\n",
        "# Add rows to the table\n",
        "for row in queryresults:\n",
        "    # Truncate each value to 20 characters\n",
        "    truncated_row = [str(value)[:20] for value in row]\n",
        "    table.add_row(truncated_row)\n",
        "\n",
        "# Print the table\n",
        "print(table)\n",
        "\n",
        "# Commit the changes\n",
        "conn.commit()\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---------------+---------+----------------------+----------------------+\n|    filename   | chunkid |        chunk         |      embedding       |\n+---------------+---------+----------------------+----------------------+\n|  resume_0.pdf |   0_0   | John Doe Education B | [-1.1703418e-002,-1. |\n|  resume_1.pdf |   1_0   | Daniel Thompson dsth | [-4.8026104e-002,1.5 |\n|  resume_1.pdf |   1_1   | 2020  Present  Techn | [-1.8442072e-002,-3. |\n| resume_18.pdf |   10_0  | Christina Taylor Con | [5.2000964e-003,-1.1 |\n| resume_19.pdf |   11_0  | Akira Tanaka Contact | [-1.0703366e-002,-6. |\n|  resume_2.pdf |   12_0  | John Doe 1234 Elm St | [-1.3231047e-002,1.6 |\n| resume_20.pdf |   13_0  | Name Johnathan Doe E | [-1.4172998e-002,5.7 |\n| resume_21.pdf |   14_0  | Alexander Johnson al | [-4.7440995e-003,-1. |\n| resume_22.pdf |   15_0  | James Reynolds Conta | [-8.5942987e-003,-6. |\n| resume_23.pdf |   16_0  | Johnathan Smith Emai | [-1.2715508e-002,1.1 |\n+---------------+---------+----------------------+----------------------+\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "azdata_cell_guid": "88d1a90e-e93c-426e-934d-fb1a47dfd900",
        "language": "python",
        "gather": {
          "logged": 1738738204438
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### **VECTOR\\_DISTANCE 함수를 이용해 벡터 유사도 검색하기**\n",
        "\n",
        "\n",
        "이제 주어진 쿼리와의 벡터 유사도가 높은 파일을 검색하는 작업을 수행할 수 있습니다.\n",
        "\n",
        "VECTOR\\_DISTANCE 함수는 쿼리의 벡터와 SQL DB에 저장된 임베딩 간의 거리를 계산합니다. 거리 계산 방식이 되는 metric으로는 cosine, euclidean, dot을 선택할 수 있습니다."
      ],
      "metadata": {
        "azdata_cell_guid": "b7b0fda6-3322-4bbc-8d08-d0b567da79ec",
        "language": "python"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyodbc\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def vector_search_sql(query, num_results=5):\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv()\n",
        "\n",
        "    # Use the get_mssql_connection function to get the connection string details\n",
        "    conn = get_mssql_connection()\n",
        "\n",
        "    # Create a cursor object\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Generate the query embedding for the user's search query\n",
        "    user_query_embedding = get_embedding(query)\n",
        "    \n",
        "    # SQL query for similarity search using the function vector_distance to calculate cosine similarity\n",
        "    sql_similarity_search = f\"\"\"\n",
        "    SELECT TOP(?) filename, chunkid, chunk,\n",
        "           1-vector_distance('cosine', CAST(CAST(? as NVARCHAR(MAX)) AS VECTOR(1536)), embedding) AS similarity_score,\n",
        "           vector_distance('cosine', CAST(CAST(? as NVARCHAR(MAX)) AS VECTOR(1536)), embedding) AS distance_score\n",
        "    FROM dbo.resumedocs\n",
        "    ORDER BY distance_score \n",
        "    \"\"\"\n",
        "\n",
        "    cursor.execute(sql_similarity_search, num_results, json.dumps(user_query_embedding), json.dumps(user_query_embedding))\n",
        "    results = cursor.fetchall()\n",
        "\n",
        "    # Close the database connection\n",
        "    conn.close()\n",
        "\n",
        "    return results\n",
        "    \n",
        "#example usage\n",
        "vector_search_sql(\"database administrator\", num_results=3)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "[('resume_84.pdf', '83_1', 'contribute to the success of your organization', 0.7804182655475509, 0.2195817344524491),\n ('resume_93.pdf', '93_0', 'Resume Name John Smith Phone 555 1234567 Email johnsmithexamplecom LinkedIn linkedincominjohnsmith Objective Highly skilled and motivated software developer with a strong foundation in computer science and extensive experience in developing testing and maintaining software applications Proficient in multiple programming languages with a keen interest in producing highquality code Education Bachelor of Science in Computer Science University of TechVille Graduated May 2018 Skills  Programming Languages Java Python C  Web Development HTML CSS JavaScript Reactjs  Frameworks Spring Boot Django Nodejs  Databases MySQL PostgreSQL MongoDB  Tools Git Docker Jenkins  Cloud Services AWS Azure  Methodologies Agile Scrum Professional Experience Software Developer ABC Technologies New York NY June 2018  Present Designed developed and implemented scalable web applications using Java and Spring Boot Collaborated with crossfunctional teams to deliver endtoend solutions and ensure project alignment with business goals Implemented RESTful APIs and integrated thirdparty services to enhance application functionality Spearheaded the migration of legacy systems to modern cloudbased architectures improving system reliability and scalability Led a team of junior developers providing mentorship and code reviews to ensure adherence to best practices and coding standards Utilized Docker and Kubernetes for containerization and orchestration achieving improved application deployment efficiency Drove the adoption of automated testing frameworks resulting in a 30 reduction in bug reports postdeployment Junior Software Developer XYZ Innovations Boston MA June 2016  May 2018 Assisted in the development and maintenance of web applications using Python and Django framework Conducted unit testing and debugging ensuring the integrity and performance of code Worked closely with senior developers to design database schemas and optimize SQL queries for better performance Contributed to frontend development tasks using HTML CSS and JavaScript enhancing user experience and interface design Participated in Agile development processes including sprint planning daily standups and retrospectives Acquired practical experience in version control using Git and implemented continuous integration practices with Jenkins Projects Ecommerce Platform Development  Developed a highly responsive and userfriendly ecommerce application using Reactjs for the frontend and Spring Boot for the backend  Implemented payment gateway integrations and optimized product search functionality using Elasticsearch Inventory Management System  Created a centralized inventory management system to facilitate realtime tracking and reporting of stock levels  Introduced automated notifications for low stock alerts significantly reducing the risk of stockouts Certifications  AWS Certified Solutions Architect  Associate  Oracle Certified Professional Java SE 11 Developer Additional Information Active contributor to opensource projects on GitHub Regular speaker at local tech meetups and conferences References Available upon request', 0.7704910805034252, 0.22950891949657481),\n ('resume_51.pdf', '47_0', 'Your Name Your Address City State Zip Your Email Your Phone Number Linkedin Profile GitHub Profile Objective Detailoriented and innovative software developer with a strong foundation in computer science principles and handson experience in fullstack development Seeking a challenging role to leverage my expertise in developing scalable and robust software applications Education Bachelor of Science in Computer Science University of California Berkeley  May 2021 Technical Skills  Programming Languages Java Python C JavaScript  Frameworks and Libraries React Nodejs Express Spring Boot Django  Databases MySQL MongoDB PostgreSQL  Tools Git Docker Kubernetes AWS Jenkins  Methodologies Agile Scrum TestDriven Development TDD Professional Experience Software Developer Tech Innovations Inc San Francisco CA June 2021  October 2023  Developed and maintained web applications using React for frontend and Nodejs with Express for backend ensuring high performance and responsiveness to user requests  Collaborated with crossfunctional teams to define project requirements deliver highquality code and troubleshoot complex issues resulting in a 20 decrease in customerreported bugs  Managed a PostgreSQL database system for application data implementing efficiency improvements that enhanced data retrieval speed by 15  Deployed applications on AWS leveraging services such as EC2 S3 and RDS to optimize performance and scalability  Employed Agile methodology to streamline workflows participate in daily standups and effectively manage sprints to ensure timely project delivery Intern  Software Development NextGen Software Solutions Berkeley CA May 2020  August 2020  Assisted in the design and development of a dynamic ecommerce application using Java and Spring Boot contributing to increased user engagement by 30  Implemented RESTful API endpoints to enable seamless integration with frontend technologies and thirdparty services  Authored unit and integration tests utilizing JUnit to ensure robustness and reliability of the codebase  Contributed to the transition from monolithic architecture to microservices enhancing the maintainability and scalability of the system Projects  Personal Portfolio Website Developed a responsive portfolio website using React and Bootstrap to showcase projects and professional achievements Deployed on GitHub Pages  Task Manager App Built a task management application with an intuitive user interface using Vuejs and Firebase providing realtime updates and user authentication Certifications  AWS Certified Developer  Associate Professional Affiliations  Association for Computing Machinery ACM Member References Available upon request', 0.7698204506385021, 0.23017954936149787)]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "azdata_cell_guid": "1b4f0ca2-2401-4f90-a44d-75dce03500cc",
        "language": "python",
        "gather": {
          "logged": 1738738236470
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# **Part 4 : SQL DB 검색 결과를 이용해 LLM 성능 강화하기 (RAG)**\n",
        "\n",
        "[Retrieval-Augmented Generation (RAG)](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview)는 외부 지식 검색을 통해 AI의 성능을 강화하는 방법입니다.\n",
        "\n",
        "이때 Retrieval은, 앞서 SQL DB에 저장해둔 벡터 임베딩의 쿼리 결과, 즉 높은 유사성의 지원자 정보라고 할 수 있습니다.\n",
        "\n",
        "이 결과를 바탕으로 Azure Open AI는 유저에게 보다 정확한 답변을 생성할 수 있게 됩니다."
      ],
      "metadata": {
        "azdata_cell_guid": "1e194f3c-6a7a-4f16-95ec-05f60a3770a4",
        "language": "python"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apim_url = os.getenv(\"APIM_URL\")\n",
        "deployment_name = os.getenv(\"APIM_CHAT_DEPLOYMENT_NAME\")\n",
        "api_version = os.getenv(\"APIM_CHAT_DEPLOYMENT_VERSION\")\n",
        "subscription_key = os.getenv(\"APIM_SUBSCRIPTION_KEY\")\n",
        "\n",
        "# Construct the URL and headers\n",
        "\n",
        "chat_url = f\"{apim_url}/deployments/{deployment_name}/chat/completions?api-version={api_version}\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Ocp-Apim-Subscription-Key\": subscription_key\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_completion(search_results, user_input):\n",
        "    system_prompt = '''\n",
        "    You are an intelligent & funny assistant who will exclusively answer based on the data provided in the `search_results`:\n",
        "    - Use the information from `search_results` to generate your top 3 responses. If the data is not a perfect match for the user's query, use your best judgment to provide helpful suggestions and include the following format:\n",
        "    File: {filename}\n",
        "    Chunk ID: {chunkid}\n",
        "    Similarity Score: {similarity_score}\n",
        "    Add a small snippet from the Relevant Text: {chunktext}\n",
        "    Do not use the entire chunk\n",
        "    - Avoid any other external data sources.\n",
        "    - Add a summary about why the candidate maybe a goodfit even if exact skills and the role being hired for are not matching , at the end of the recommendations. Ensure you call out which skills match the description and which ones are missing. If the candidate doesnt have prior experience for the hiring role which we may need to pay extra attention to during the interview process.\n",
        "    - Add a Microsoft related interesting fact about the technology that was searched \n",
        "    '''\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    \n",
        "    # Create an empty list to store the results\n",
        "    result_list = []\n",
        "\n",
        "    # Iterate through the search results and append relevant information to the list\n",
        "    for result in search_results:\n",
        "        filename = result[0]  # Assuming filename is the first column\n",
        "        chunkid = result[1]\n",
        "        chunktext = result[2]\n",
        "        similarity_score = result[3]  # Assuming similarity_score is the third column\n",
        "\n",
        "        # Convert search results into a formatted text block\n",
        "        result_text = \"\\n\".join([\n",
        "            f\"File: {filename}\\nChunk ID: {chunkid}\\nSimilarity Score: {similarity_score}\\nRelevant Text: {chunktext}\"\n",
        "            for result in search_results\n",
        "        ])\n",
        "\n",
        "    \n",
        "    messages.append({\"role\": \"user\", \"content\": result_text})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    json_payload = {\"messages\": messages}  # Wrap messages inside a dictionary\n",
        "\n",
        "    response = requests.post(chat_url, headers=headers, json=json_payload)\n",
        "\n",
        "    return response.json()['choices'][0]['message']['content']\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1738740851665
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loop of user input and model output to perform Q&A on the PDF's that are now chunked and stored in the SQL DB with embeddings\n",
        "#\n",
        "# PLEASE NOTE: An input box will be displayed for the user to enter a question/query at the top of the scree.\n",
        "# The model will then provide a response based on the data stored in the SQL DB.\n",
        "# Type 'end' to end the session.\n",
        "#\n",
        "\n",
        "print(\"*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User prompt: \")\n",
        "    if user_input.lower() == \"end\":\n",
        "        break\n",
        "\n",
        "    # Print the user's question\n",
        "    print(f\"\\nUser asked: {user_input}\")\n",
        "\n",
        "  \n",
        "    # Assuming vector_search_sql and generate_completion are defined functions that work correctly\n",
        "    search_results = vector_search_sql(user_input)\n",
        "    completions_results = generate_completion(search_results, user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"\\nSmart matching results:\")\n",
        "    print(completions_results)\n",
        "\n",
        "# The loop will continue until the user types 'end'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\n\n\nUser asked: project manager\n\nSmart matching results:\nHere are the top 3 relevant chunks from the search results regarding a candidate suitable for a Project Manager role:\n\n1. \nFile: resume_6.pdf\nChunk ID: 56_0\nSimilarity Score: 0.7948388558350712\nRelevant Text: \n\"Participated in daily standups and contributed to sprint planning leading to timely project delivery Wrote unit tests and integrated testing procedures maintaining high code quality standards Collaborated with the UIUX team to improve the user interface and experience for customerfacing applications.\"\n\n2. \nFile: resume_6.pdf\nChunk ID: 56_0\nSimilarity Score: 0.7948388558350712\nRelevant Text: \n\"Lead the design and development of various modules using Java and Python Collaborated with crossfunctional teams to define software specifications and architecture Implemented RESTful APIs that enhanced the performance and scalability of existing systems by 30 Conducted thorough code reviews and applied best practices for version control and continuous integration Mentored junior developers increasing team productivity and skill levels.\"\n\n3. \nFile: resume_6.pdf\nChunk ID: 56_0\nSimilarity Score: 0.7948388558350712\nRelevant Text: \n\"Assisted in developing web applications using JavaScript frameworks including React and Angular Participated in daily standups and contributed to sprint planning leading to timely project delivery.\"\n\n### Summary and Recommendations:\n\n**Strengths Matching the Project Manager Role:**\n- **Team Collaboration:** John has demonstrated a significant ability to collaborate with cross-functional teams and improve team productivity and skill levels, which are crucial for a Project Manager.\n- **Leadership Skills:** He has mentored junior developers, indicating his capability to lead and manage a team.\n- **Project Delivery:** His experience in sprint planning and contributing to project delivery timelines showcases his ability to manage projects efficiently.\n\n**Skills/Experience Missing for the Project Manager Role:**\n- **Specific Project Management Experience:** John does not explicitly mention any direct experience in a project management role or responsibilities such as project budgeting, client communication, or risk management.\n\n**Potential Extra Attention During Interview:**\n- Confirm if John has any formal project management training or certification (like PMP) which is not mentioned in the resume.\n- Verify his experience with project management tools and methodologies beyond Agile.\n\n### Microsoft Interesting Fact:\nDid you know that Microsoft Project was one of the company's first versions of a project management tool, launched in 1984 for DOS and later became part of the Microsoft Office suite? It continues to be a widely used tool in project management today, helping teams plan, schedule, and manage projects.\n\nUser asked: technical specialist\n\nSmart matching results:\nHere are my top 3 responses for the position of a Technical Specialist based on the provided resume:\n\n**1.**\nFile: resume_24.pdf\nChunk ID: 17_0\nSimilarity Score: 0.7872021193443492\nRelevant Text: \"Highly skilled software developer with 5 years of experience specializing in full-stack web development and cloud-based applications... Seeking a challenging role to leverage and expand my technical capabilities... Implemented microservices architecture for enterprise-level applications enhancing system reliability and performance... Utilized Docker and Kubernetes for containerization and orchestration of applications in a cloud environment.\"\n\n**2.**\nFile: resume_24.pdf\nChunk ID: 17_0\nSimilarity Score: 0.7872021193443492\nRelevant Text: \"Led a team of 5 developers in building scalable web applications using MERN stack (MongoDB, Express.js, React, Node.js)... Designed and integrated RESTful APIs to facilitate communication between frontend and backend services... Conducted code reviews and provided mentorship enhancing team productivity and code quality.\"\n\n**3.**\nFile: resume_24.pdf\nChunk ID: 17_0\nSimilarity Score: 0.7872021193443492\nRelevant Text: \"Collaborated with cross-functional teams to gather and define requirements resulting in improved user-centric design... Applied modern CI/CD practices using Jenkins and Git for automated deployments and version control... Addressed and resolved technical issues and bugs ensuring system robustness and reliability.\"\n\n### Summary\nJohnathan T Smith appears to be a strong candidate for a Technical Specialist role given his robust experience in full-stack web development, cloud-based applications, and modern CI/CD practices. His background in microservices architecture, containerization with Docker and Kubernetes, and mentoring a team of developers aligns well with the technical leadership and innovative expectations of a Technical Specialist.\n\n**Skills match:**\n- Full-stack web development (React, Node.js, Angular, .NET Core)\n- Cloud-native applications and containerization (Docker, Kubernetes, AWS, Azure)\n- Microservices architecture and API integration\n- Comprehensive CI/CD pipeline management using Jenkins and Git\n\n**Skills to pay extra attention to during interview:**\n- No direct mention of specific tools or processes unique to a Technical Specialist role (e.g., deeper understanding of various tech stacks, troubleshooting complex systems under high pressure)\n- Ensure proficiency in technology solutions unique to the company's stack\n\n### Fun Fact About Microsoft Technology\nDid you know? The Microsoft Azure cloud platform supports a broad range of programming languages, databases, operating systems, and devices – making it the go-to cloud service for developers who want flexibility and choice in building and deploying applications. Azure Kubernetes Service (AKS) simplifies deploying a managed Kubernetes cluster in Azure by offloading much of the complexity and operational overhead related to infrastructure management.\n\nUser asked: safety officer\n\nSmart matching results:\nIt seems that we do not have an exact match for a \"Safety Officer\" position in the given search results. However, we can consider candidates with a good background in other areas who might have transferable skills. Here is the top resume from the search results:\n\n### Recommendation 1:\n**File:** resume_93.pdf  \n**Chunk ID:** 93_0  \n**Similarity Score:** 0.7370621682725521  \n**Relevant Text Snippet:**\nHighly skilled and motivated software developer with a strong foundation in computer science... Professional Experience: Software Developer at ABC Technologies. \n\n### Summary of candidate skills for a Safety Officer role:\n**Matching Skills:**\n- Strong foundation in project management and leadership skills while leading a team of junior developers.\n- Experience implementing best practices and ensuring adherence to coding standards can translate to ensuring adherence to safety protocols.\n\n**Missing Skills:**\n- Direct experience with safety protocols and regulations.\n- Specific safety officer or industrial safety certifications.\n\n**Interview Focus Areas:**\n- Assess the candidate's understanding and implementation of safety practices in their previous roles.\n- Gauge the candidate's capacity for learning and managing regulatory requirements.\n\n### Microsoft Related Fact:\n**Fun Fact:** Microsoft Azure, which is mentioned in the resume, is one of the fastest-growing cloud platforms, with a significant number of Fortune 500 companies utilizing its services.\n\nThis developer may be a good fit if the emphasis is on strong organizational, project management, and leadership skills. If the candidate is adaptable and a quick learner, they might be able to transition effectively into a Safety Officer role with additional on-the-job training and certification.\n\nUser asked: technology sales\n\nSmart matching results:\nIt appears the provided data does not have a perfect match for a technology sales role. However, I can still highlight relevant aspects and suggest why this candidate, although primarily from a software development background, may be a good fit. Here are the top responses based on the provided information:\n\n1. **File: resume_6.pdf**\n   - **Chunk ID**: 56_0\n   - **Similarity Score**: 0.783301556262443\n   - **Relevant Text**: \"Software Development professional with 4 years of experience in designing and implementing scalable software solutions... Lead the design and development of various modules using Java and Python...\"\n   - **Snippet**: \"Proficient in modern programming languages and agile methodologies. Lead the design and development of modules, collaborated with cross-functional teams, implemented RESTful APIs, and mentored junior developers.\"\n   \n2. **File: resume_6.pdf**\n   - **Chunk ID**: 56_0\n   - **Similarity Score**: 0.783301556262443\n   - **Relevant Text**: \"Assisted in developing web applications using JavaScript frameworks including React and Angular... Wrote unit tests and integrated testing procedures maintaining high code quality standards.\"\n   - **Snippet**: \"Participated in daily standups and contributed to sprint planning, collaborated with the UI/UX team to improve user interface and experience.\"\n\n3. **File: resume_6.pdf**\n   - **Chunk ID**: 56_0\n   - **Similarity Score**: 0.783301556262443\n   - **Relevant Text**: \"Ecommerce Platform Modernization Roles Lead Developer Technologies Java Spring Boot React MySQL  Revamped a legacy ecommerce platform to a modern microservices architecture...\"\n   - **Snippet**: \"Developed a real-time analytics dashboard providing actionable insights with interactive visualizations, enhancing load handling capacity by 50%.\"\n\n### Summary of Fit:\n**Skills Matching**:\n- **Technical Skills**: Proficiency in modern programming languages (Java, Python, JavaScript) and frameworks (React, Angular, Spring Boot) which can be beneficial for understanding technology products that the sales team would be selling.\n- **Collaboration and Leadership**: Working with cross-functional teams, mentoring junior developers, and collaborating with UI/UX teams show strong interpersonal and leadership skills.\n- **Project Management**: Experience in sprint planning and leading development projects highlights project management capabilities.\n\n**Skills Missing**:\n- **Sales Experience**: The candidate's resume does not reflect direct experience in technology sales.\n- **Customer Facing Experience**: Limited information on experience in a customer-facing role or in business development.\n\nWhile John Doe has a strong technical background, additional focus during the interview process should be on assessing his communication skills, understanding of sales processes, and ability to effectively bridge the gap between technical knowledge and customer needs.\n\n### Microsoft Fun Fact:\nDid you know that Microsoft’s cloud platform, Azure, is growing faster than Amazon Web Services and Google Cloud? Azure's extensive range of services and global reach make it a strong contender in the cloud market, supported by Microsoft's continuous innovation and integration across its product lines!\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "azdata_cell_guid": "d69ab285-0cc9-4596-95f2-688d0c324f6b",
        "language": "python",
        "tags": [],
        "gather": {
          "logged": 1738741023526
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}